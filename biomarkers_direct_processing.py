# -*- coding: utf-8 -*-
"""biomarkers_direct_processing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EGpQE-PVyiZ1R0ssvN2Wc5ZojUCzg0zm
"""

# @title Setup
import numpy as np
import pandas as pd
import json
import matplotlib.pyplot as plt
import pytz
import datetime as dt
from datetime import timedelta

from google.colab import auth
from google.cloud import bigquery
from google.colab import data_table

project = 'willow-health' # Project ID inserted based on the query results selected to explore
location = 'US' # Location inserted based on the query results selected to explore
client = bigquery.Client(project=project, location=location)
data_table.enable_dataframe_formatter()
auth.authenticate_user()

from google.colab import drive
drive.mount('/content/drive')

"""# Result set loaded from BigQuery job as a DataFrame

Use the ```jobs.query```
[method](https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs/query) to
return the SQL syntax from the job. This can be copied from the output cell
below to edit the query now or in the future. Alternatively, you can use
[this link](https://console.cloud.google.com/bigquery?j=willow-health:US:bquxjob_3edd4002_187b91d2982)
back to BigQuery to edit the query within the BigQuery user interface.

Query results are referenced from the Job ID ran from BigQuery and the query
does not need to be re-run to explore results. The ```to_dataframe```
[method](https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJob.html#google.cloud.bigquery.job.QueryJob.to_dataframe)
downloads the results to a Pandas DataFrame by using the BigQuery Storage API.

To edit query syntax, you can do so from the BigQuery SQL editor or in the
```Optional:``` sections below.
"""

# Running this code will display the query used to generate your previous job

# Run the following code to get the latest JOB ID from Cloud Shell Terminal:
# bq ls -j -a --max_results=1 --format=prettyjson | jq '.[0].jobReference.jobId'

LOAD_LOCAL = True # CHANGE THIS TO FALSE TO RUN FROM CLOUD

if LOAD_LOCAL:
  results = pd.read_csv("/content/drive/MyDrive/AI and Mental Health Project/MIT_Pilot_Analysis/RealTime_sleep_survey_stand_raw_latest_05_07_2023.csv")
  results_hr = pd.read_csv("/content/drive/MyDrive/AI and Mental Health Project/MIT_Pilot_Analysis/New_Sample_Users_RealTime_hours_raw_latest_05_07_2023.csv")
else:
  JOB_ID = 'bquxjob_447725cc_187f817a37c' # Sleep
  JOB_HR_ID = 'bquxjob_6b51a974_187f8183f3c' # Heart Rate Info

  # Running this code will read results from your previous job

  job_realtime_sleep_survey = client.get_job(JOB_ID) # Job ID inserted based on the query results selected to explore
  print(job_realtime_sleep_survey.query)
  results = job_realtime_sleep_survey.to_dataframe()

  job_realtime_hr = client.get_job(JOB_HR_ID)
  print(job_realtime_hr.query)
  results_hr = job_realtime_hr.to_dataframe()

# # Save data locally
# results.to_csv("/content/drive/MyDrive/AI and Mental Health Project/MIT_Pilot_Analysis/RealTime_sleep_survey_stand_raw_latest_05_07_2023.csv")
# results_hr.to_csv("/content/drive/MyDrive/AI and Mental Health Project/MIT_Pilot_Analysis/New_Sample_Users_RealTime_hours_raw_latest_05_07_2023.csv")

# @title Helper Preprocessing Functions

# Define a function to parse the JSON data
def parse_json(x):
    try:
        return json.loads(x.replace("'", "\""))
    except:
        return {}

def output_case_check(x):
  if isinstance(x, list):
    if len(x) > 0:
      return x[0]
    else:
      return None
  else:
    return x

def subjson_processing(result_df, col_name, explode=False):
    # Do the preprocessing of sub-string with json data
    result_df[col_name] = result_df[col_name].apply(output_case_check)
    if explode:
      result_df = result_df.explode(col_name,ignore_index=True)
    col_name_result = pd.json_normalize(result_df[col_name])
    print(col_name_result.columns)
    return pd.concat([result_df.drop(col_name,axis=1), col_name_result], axis=1)
    print(result_df.columns)


def old_preprocess(df, category= "sleep_analysis"):
  # Apply the function to the "data" column to create a new DataFrame with the normalized data
  normalized_df = pd.json_normalize(df['data'].apply(parse_json))

  # Concatenate the normalized data with the original DataFrame
  result_df = pd.concat([df, normalized_df], axis=1)

  # Get user_id from document_name
  result_df['user_id'] = result_df['document_name'].str.split('/').str[6]

  # Get hours from document_name
  result_df['date'] = result_df['document_name'].str.split('/').str[8]

  # Prettify column names generated from JSON keys
  result_df.columns = result_df.columns.str.split('.').str[-1]

  # Run preprocessing for sleep_analysis, surveys, or other data formats
  if category == "sleep_analysis":
    # Drop irrelevant columns
    result_df = result_df[['user_id', 'date', 'sleep_analysis']]
    result_df.rename(columns={'user_id':'userid'},inplace=True)

    # Explode list of sleep markers and extract json infomation
    result_df = result_df.explode('sleep_analysis',ignore_index=True)
    all_sleep = pd.json_normalize(result_df['sleep_analysis'])

    result_df = pd.concat([result_df.drop('sleep_analysis',axis=1), all_sleep], axis=1)
    result_df = result_df.query("sleep_type == 'in_bed'")

  elif category == "survey":
    result_df = result_df[['user_id', 'date', 'document_id', 'data',"survey_name", "questions_answers"]]
    result_df.rename(columns={'user_id':'userid','document_id':'hour'},inplace=True)
    result_df = subjson_processing(result_df, "survey_name")
    result_df = result_df.drop(columns='data').explode(column='questions_answers',ignore_index=True)
    result_df = subjson_processing(result_df, "questions_answers")

  else:
    result_df.rename(columns={'user_id':'userid','document_id':'hour'},inplace=True)

  return result_df

"""# Visualization of Sleep Features"""

df_with_sleep = old_preprocess(results,'sleep_analysis')
df_with_sleep['date'] = pd.to_datetime(df_with_sleep['date'])
df_with_sleep['start_time'] = pd.to_datetime(df_with_sleep['start_time'])
df_with_sleep['end_time'] = pd.to_datetime(df_with_sleep['end_time'])

df_with_sleep.sort_values(by='date',ascending=False)

# @title Helper Sleep Marker Consolidation

def in_bed_consolidator(user_target_sleep):
  """ This simplifies the in_bed marker by taking the latest end_time,
  then the earliest start_time, and subtracting the two to get that night's
  sleep estimate"""

  max_end_time = user_target_sleep['end_time'].max()
  min_start_time = user_target_sleep['start_time'].min()
  total_sleep = (max_end_time - min_start_time)/pd.Timedelta(hours=1)

  # Create a single row with the new consolidated sleep values
  new_entry = pd.DataFrame({'userid':user_target_sleep['userid'].iloc[0], 'date':user_target_sleep['date'].iloc[0],
                            'end_time':max_end_time, 'sleep_type':user_target_sleep['sleep_type'].iloc[0],
                            'start_time':min_start_time, 'sleep_duration': max_end_time - min_start_time,
                            'sleep_duration_hours': total_sleep,
                            'start_since_midnight': user_target_sleep['start_since_midnight'].iloc[0]},
                            index=[user_target_sleep.index[0]])
  return new_entry

def diff_calculator(x):
  new_x = x.copy()
  new_x['sleep_time_diff'] = new_x['sleep_time_diff'].diff().abs()
  return new_x

def tz_change(x):
  """Function to change time zone from UTC to EST"""
  x['start_time'] = x['start_time'].tz_localize('UTC').tz_convert('US/Eastern')
  x['end_time'] = x['end_time'].tz_localize('UTC').tz_convert('US/Eastern')
  return x

# Clean any row that does not have a start_time value
sleep_analysis_df = df_with_sleep.query("sleep_type == 'in_bed'").dropna(subset=['start_time'])
# Clean any duplicates that have the same end_time, start_time, and sleep_type
sleep_analysis_df = sleep_analysis_df.drop_duplicates(subset=['end_time','sleep_type',
                    'start_time'],keep='first').sort_values(by='date',ascending=False)

# Time zone change from UTC to EST (NOT necessary but helps visualize for EST people)
sleep_analysis_df= sleep_analysis_df.apply(tz_change,axis=1)

# Calculating sleep duration in minutes
sleep_analysis_df['sleep_duration'] = sleep_analysis_df['end_time'] - sleep_analysis_df['start_time']
sleep_analysis_df['sleep_duration_hours'] = sleep_analysis_df['sleep_duration'] / pd.Timedelta(hours=1)
sleep_analysis_df['start_since_midnight'] = (sleep_analysis_df["start_time"] - sleep_analysis_df['start_time'].dt.normalize()) / pd.Timedelta(hours=1)

# Process sleep data by consolidating multiple in_bed markers
processed_sleep_df = sleep_analysis_df.groupby(by=['date','userid'],as_index=False).apply(in_bed_consolidator).reset_index(drop=True)
# Eliminate any erroneous (too high) sleep values
processed_sleep_df = processed_sleep_df.query("sleep_duration_hours < 14")

# Set up sleep_time_diff as a column to calculate sleep onset time difference
processed_sleep_df['sleep_time_diff'] = processed_sleep_df['start_since_midnight'].copy()
processed_sleep_df['sleep_time_diff'][processed_sleep_df['sleep_time_diff'] < 12]  += 24
processed_sleep_df = processed_sleep_df.groupby(by=['userid'],group_keys=False).apply(diff_calculator)

processed_sleep_df.head()

start_date = pd.Timestamp('2023-04-01 00:00:00')
processed_sleep_april = processed_sleep_df.query("date > @start_date")

colors = {participant:f"C{idx}" for idx,participant in enumerate(processed_sleep_april['userid'].unique())}
fig, axes = plt.subplots(int(np.ceil(len(processed_sleep_april['userid'].unique())/2)),2, sharex=True,figsize=(12,12))
axes = axes.flatten()

for idx,u in enumerate(processed_sleep_april['userid'].unique()):
  current_df = processed_sleep_april.query("userid == @u")
  axes[idx].bar(current_df["date"], current_df["sleep_duration_hours"], color=colors[u], label=u[:4], alpha=0.5)
  axes[idx].set(xlabel="Date", ylabel="Sleep Duration")
  axes[idx].legend()

plt.tight_layout()

colors = {participant:f"C{idx}" for idx,participant in enumerate(processed_sleep_april['userid'].unique())}
fig, axes = plt.subplots(int(np.ceil(len(processed_sleep_april['userid'].unique())/2)),2, sharex=True,figsize=(12,12))
axes = axes.flatten()

for idx,u in enumerate(processed_sleep_april['userid'].unique()):
  current_df = processed_sleep_april.query("userid == @u")
  axes[idx].bar(current_df["date"], current_df["start_time"].dt.hour, color=colors[u], label=u[:4], alpha=0.5)
  axes[idx].set(xlabel="Date", ylabel="Sleep Onset")
  axes[idx].legend()

colors = {participant:f"C{idx}" for idx,participant in enumerate(processed_sleep_april['userid'].unique())}
fig, axes = plt.subplots(int(np.ceil(len(processed_sleep_april['userid'].unique())/2)),2, sharex=True,figsize=(12,12))
axes = axes.flatten()

for idx,u in enumerate(processed_sleep_april['userid'].unique()):
  current_df = processed_sleep_april.query("userid == @u").dropna(subset=['sleep_time_diff'])
  axes[idx].bar(current_df["date"], current_df["sleep_time_diff"], color=colors[u], label=u[:4], alpha=0.5)
  axes[idx].set(xlabel="Date", ylabel="Sleep Time Change")
  axes[idx].legend()

plt.tight_layout()

# # Scratch code to test sleep consolidator
# user_sleep = sleep_analysis_df.query("userid == 'BUjB4KojShMKb37PR855ih5Bhz03'")
# sleep_analysis_df.query("userid == 'BUjB4KojShMKb37PR855ih5Bhz03'").groupby(by=['date','userid'],as_index=False).apply(in_bed_consolidator).reset_index(drop=True)
# target_date = pd.Timestamp('2023-04-17 00:00:00')
# user_target_sleep = user_sleep.query("date == @target_date")
# in_bed_consolidator(user_target_sleep)

# # Test sleep data marker

# # Test if sleep "in_bed" marker date matches up with assumptions about where sleep time goes
# date_interest = pd.Timestamp('2023-04-20 00:00:00', tz='US/Eastern')
# date_upper_bound = date_interest + pd.Timedelta(hours=12)
# date_lower_bound = date_interest - pd.Timedelta(days=1) + pd.Timedelta(hours=12+5)
# user_sleep.query("end_time < @date_upper_bound and start_time > @date_lower_bound")

# # Look at the date for each "in_bed" marker to test this assumption
# target_date = pd.Timestamp('2023-04-20 00:00:00')
# user_sleep.query("date == @target_date")

"""# Visualize Survey Response and Sleep Features

Plan features to analyze:
1. Visualize heart rate features
2. Visualize heart rate variability features
3. Visualize survey ratings
4. Visualize heart rate or heart rate variability and survey features
5. Visualize heart rate variability or heart rate (taking into account sleep timing)
"""

df_with_survey = old_preprocess(results,'survey')
df_with_survey['date'] = pd.to_datetime(df_with_survey['date'])
df_with_survey = df_with_survey.sort_values(by='date',ascending=False)

df_with_survey['identifier'].value_counts()
survey_sleep_merge = pd.merge(processed_sleep_df,df_with_survey, on=["date","userid"])

df_with_survey

# @title Simple Data Exploration (Optional)
# # Create query that only keeps the users specified

# for id in ['relax','sensitive','stressed','agitated','windDown']:
#   id_df = survey_sleep_merge.query("identifier == @id")
#   plt.figure()
#   for u in ['Fv4B4vK1pHZ0SsQyzEGcPIYhWyp1','BUjB4KojShMKb37PR855ih5Bhz03',
#     'Iayn6M5zlOXwkV8AEtteKAXvsFj2', 'KlbV78QFcnQUOFwXtqu8fsT6t7Y2', 'QSqRQYYpSXS8TnJmsIHtW6JD5UF2']:
#     user_id_df = id_df.query("userid == @u")
#     plt.scatter(user_id_df['user_answer'],user_id_df['sleep_duration_hours'],label=u[:4])
#     plt.title(f"Sleep Duration {id}")
#     plt.xlim([-0.2,5.2])
#   plt.legend()

# # Create query that only keeps the users specified

# for id in ['relax','sensitive','stressed','agitated','windDown']:
#   id_df = survey_sleep_merge.query("identifier == @id").dropna(subset=['sleep_time_diff'])
#   plt.figure()
#   for u in ['Fv4B4vK1pHZ0SsQyzEGcPIYhWyp1','BUjB4KojShMKb37PR855ih5Bhz03',
#     'Iayn6M5zlOXwkV8AEtteKAXvsFj2', 'KlbV78QFcnQUOFwXtqu8fsT6t7Y2', 'QSqRQYYpSXS8TnJmsIHtW6JD5UF2']:
#     user_id_df = id_df.query("userid == @u")
#     plt.scatter(user_id_df['user_answer'],user_id_df['sleep_time_diff'],label=u[:4])
#     plt.title(f"Sleep Diff {id}")
#     plt.xlim([-0.2,5.2])
#   plt.legend()

"""# Visualize HRV Features vs. Survey Response"""

df_with_hrv = old_preprocess(results_hr, 'neither')
df_with_hrv['date'] = pd.to_datetime(df_with_hrv['date'])

relevant_cols = ['userid', 'date', 'hour',
                 'heart_rate_avg', 'heart_rate_max', 'heart_rate_min',
                 'resting_heart_rate_avg', 'resting_heart_rate_max', 'resting_heart_rate_min',
                 'heart_rate_variability_sdnn_max', 'heart_rate_variability_sdnn_min',
                 'heart_rate_variability_sdnn_avg']

since = pd.Timestamp('2023-03-01')

user_df = df_with_hrv.query("date > @since").sort_values(by=["date","hour"])[relevant_cols]
unique_users = user_df['userid'].unique()

# Creating dataframe with aggregated average HRV values
hrv_user_df = user_df.dropna(subset=['heart_rate_variability_sdnn_avg'])
hrv_group_by_date = hrv_user_df.groupby(["date","userid"], as_index=False).mean()

# Creating dataframe with aggregated average HR values
hr_user_df = user_df.dropna(subset=["heart_rate_avg"])
hr_user_df['date_hour'] = pd.to_datetime(hr_user_df['date'].astype(str) + ' '
                                         + hr_user_df['hour'], utc=True
                                         ).dt.tz_convert('US/Eastern')
hr_group_by_date = hr_user_df.groupby(["date","userid"], as_index=False).mean()

# @title Visualize HR and HRV over time (Optional)

# # Plotting each user's stats:
# for user in user_df['userid'].unique():
#   if len(hrv_group_by_date.query("userid == @user")) > 1:
#     fig, axes = plt.subplots(2,1,figsize=(12,6))
#     all_axes = axes.flatten()

#     hrv_group_by_date.query("userid == @user").plot(x="date",y="heart_rate_variability_sdnn_avg",
#                                                     ax=all_axes[0],marker='o')
#     hr_group_by_date.query("userid == @user").plot(x="date",y=["heart_rate_avg","heart_rate_min",
#                                                     "heart_rate_max"],ax=all_axes[1],marker='o')
#     all_axes[0].set_title(f"User {user[:4]}")

# # stress_user_df['userid'].unique()
# since = pd.Timestamp('2023-04-01')
# stress_user_df.sort_values(by='date').query("date > @since")

# # hrv_group_by_date['userid'].unique()
# since = pd.Timestamp('2023-04-01')
# hrv_group_by_date.sort_values(by='date').query("date > @since")

survey_user_df = df_with_survey.sort_values(by="date").dropna(subset=["question"])
# stress_user_df = survey_user_df.query("identifier == 'stressed'").reset_index(drop=True)
stress_user_df = survey_user_df.query("identifier == 'stressed'")

# DASS_questions = ['sensitive','stressed', 'windDown', 'relax', 'agitated']
# avg_stress_user_df = survey_user_df.query(' or '.join([f"identifier == '{q}'" for q in DASS_questions]))
# stress_user_df['avg_user_answer'] = avg_stress_user_df.groupby(by=['userid','date'],as_index=False).mean()['user_answer']

# Merge the HRV and survey stress data based on date and user
hrv_vs_stress_df = pd.merge(hrv_group_by_date,stress_user_df.drop(["identifier"],axis=1), on=["date","userid"])
hrv_colors = {participant:f"C{idx}" for idx,participant in enumerate(hrv_vs_stress_df['userid'].unique())}

# Merge the HR and survey stress data based on date and user
hr_vs_stress_df = pd.merge(hr_group_by_date,stress_user_df.drop(["identifier"],axis=1), on=["date","userid"])
hr_colors = {participant:f"C{idx}" for idx,participant in enumerate(hr_vs_stress_df['userid'].unique())}

plt.figure()

for u in hrv_vs_stress_df['userid'].unique():
  current_df = hrv_vs_stress_df.query("userid == @u")
  plt.scatter(current_df["user_answer"], current_df["heart_rate_variability_sdnn_max"], c=hrv_colors[u], label=u[:4])
  plt.xlabel("User Answer"), plt.ylabel("Heart Rate Variability Average")

plt.legend()

plt.figure()

for u in hr_vs_stress_df['userid'].unique():
  current_df = hr_vs_stress_df.query("userid == @u")
  plt.scatter(current_df["user_answer"], current_df["heart_rate_avg"], c=hr_colors[u], label=u[:4])
  plt.xlabel("User Answer"), plt.ylabel("Heart Rate Average")

plt.legend()

"""# Model Building and Feature Engineering

1. Implement custom features (custom equations) for heart rate variability or heart rate (taking into account sleep timing)
2. Build a model (linear regression model, decision tree, random forest, or mixed effect random forest)
  1. Linear regression model and regression model: more interpretable
  2. Random forest or mixed effect random forest: likely better performance but less interpretability
  3. (potentially) principal component

Feature ideas: sleep consistency in last 2 days
"""

# Linear Regression and Pearson Correlation
from sklearn import (datasets, linear_model, feature_selection,
                     model_selection, metrics, utils, preprocessing,
                     svm)

from sklearn.linear_model import LinearRegression
import statsmodels.api as sm
from scipy import stats
import seaborn as sns

# @title Cross-validation and Parameter Tuning Functions

def kfold_splits(X,Y):
  X_shufled, Y_shufled = utils.shuffle(X,Y,random_state=0)

  kf = model_selection.KFold(5)

  splits_ls = []
  for i, (train_index, test_index) in enumerate(kf.split(X_shufled)):
    X_train, y_train = X_shufled[train_index], Y_shufled[train_index]
    X_test, y_test = X_shufled[test_index], Y_shufled[test_index]
    splits_ls.append((X_train,y_train,X_test,y_test))

  return splits_ls

def sfs_selection(model,X,Y):
  sfs = feature_selection.SequentialFeatureSelector(model,n_features_to_select="auto")
  sfs.fit(X,Y)
  return sfs

def param_tuning(X, Y, model_func, param_dict, verbose=False, sfs= False, input_features=None):
  '''
  model_func: the function (i.e. linear_model.Lasso) without initializing or calling
  param_dict: a dictionary containing a list of dictionaries with parameters to try. For example,
    [{'alpha':0.1,'fit_intercept':True}, {'alpha':0.01,'fit_intercept':True}]
  '''
  models_ls = []
  sfs_ls = []

  for param in param_dict:
    # Create new model with param
    model = model_func(**param)
    loss_ls = []
    acc_ls = []

    if sfs: # Perform feature selection through Sequential Forward Selection
        sfs_selector = sfs_selection(model,X,Y)
        sfs_ls.extend(sfs_selector.get_feature_names_out(input_features))

    # Preform k-fold corss validation
    for X_train, y_train, X_test, y_test in kfold_splits(X,Y):

      if sfs: # Transform training and testing set to keep only selected features
        X_train = sfs_selector.transform(X_train)
        X_test = sfs_selector.transform(X_test)

      model.fit(X_train,y_train)
      y_pred = model.predict(X_test)
      loss_ls.append(metrics.mean_squared_error(y_pred,y_test))
      acc_ls.append(metrics.accuracy_score(np.ceil(y_pred).astype(int),y_test))

    # Create a list of each model created which will be sorted by minimum loss
    if verbose:
      # print("y_pred", y_pred)
      print("Avg loss: ",np.mean(loss_ls)," Accuracy: ", np.mean(acc_ls))
    models_ls.append({"acc":np.mean(acc_ls),"mse":np.mean(loss_ls),"model":model,
                      "sfs":sfs_selector if sfs else None})

  if sfs:
    unique_features = np.unique(sfs_ls, return_counts=True)
    print({feature:count for feature,count in zip(*unique_features)})

  return sorted(models_ls,key=lambda x: (1-x["acc"],x["mse"]))[0]

"""## Model Building for HR Features"""

# Set up feature vectors for analysis
independent_variables = ['heart_rate_avg', 'heart_rate_max', 'heart_rate_min',
       'resting_heart_rate_avg', 'heart_rate_variability_sdnn_avg']

np.random.seed(5)

hr_features_df = hr_vs_stress_df[independent_variables].fillna(method='ffill',axis=0).fillna(method='bfill',axis=0)
X = hr_features_df.to_numpy()
X_norm = (X - np.mean(X, axis=0))/np.std(X, axis=0)

# Generating label based on 0-5 score or binary for stressed or not stressed
Y = hr_vs_stress_df['user_answer'].astype(int).to_numpy()
Y_binary = Y.copy()
Y_binary[Y_binary < 2.5] = 0
Y_binary[Y_binary > 2.5] = 1

sns.pairplot(hr_vs_stress_df[independent_variables + ['user_answer']], kind="reg", diag_kind="kde")

# Run Ordinary Least Squares to obtain coefficients and Statistically Singficiant Features
X2 = sm.add_constant(X)
est = sm.OLS(Y, X2)
est2 = est.fit()
print(est2.summary())

# Create linear regression model with regularization (LASSO) and find best parameter
clf = linear_model.Lasso
clf_test_params = [{"alpha":a} for a in [0.01,0.05,0.1,0.2,0.5,0.8]]

clf_model_hr = param_tuning(X, Y, clf, clf_test_params, verbose=True)

# Test Lasso model with binary stress lable (>2.5)
clf_model_hr_binary = param_tuning(X, Y_binary, clf, clf_test_params, verbose=True)

# SVM and SequentialForwardSelection
svm_linear = svm.SVC

svm_test_params = [{"C":c,"kernel":k}
                   for c in [0.01,0.1,1,2,5]
                   for k in ['linear', 'rbf','sigmoid']]

svm_model_hr = param_tuning(X, Y, svm_linear,
                        svm_test_params, verbose=True, sfs=True,
                        input_features=independent_variables)

# Test SVM model with binary stress lable (>2.5)
svm_model_hr_binary = param_tuning(X, Y_binary, svm_linear,
                                svm_test_params, verbose=True, sfs=True,
                                input_features=independent_variables)

"""## Model Building Sleep Features"""

# Set up feature vectors for analysis
independent_variables = ['sleep_duration_hours', 'sleep_time_diff']


np.random.seed(5)
stress_survey_sleep_merge = survey_sleep_merge.query("identifier == 'stressed'").dropna(subset=['user_answer'])

sleep_features_df = stress_survey_sleep_merge[independent_variables].fillna(method='ffill',axis=0).fillna(method='bfill',axis=0)
X = sleep_features_df.to_numpy()

# Generating label based on 0-5 score or binary for stressed or not stressed
Y = stress_survey_sleep_merge['user_answer'].astype(int).to_numpy()
Y_binary = Y.copy()
Y_binary[Y_binary < 2.5] = 0
Y_binary[Y_binary > 2.5] = 1

sns.pairplot(stress_survey_sleep_merge[independent_variables + ['user_answer']], kind="reg", diag_kind="kde")

# Run Ordinary Least Squares to obtain coefficients and Statistically Singficiant Features
X2 = sm.add_constant(X)
est = sm.OLS(Y, X2)
est2 = est.fit()
print(est2.summary())

# Create linear regression model with regularization (LASSO) and find best parameter
clf = linear_model.Lasso
clf_test_params = [{"alpha":a} for a in [0.01,0.05,0.1,0.2,0.5,0.8]]

clf_model_sleep = param_tuning(X, Y, clf, clf_test_params, verbose=True)

# Test regression model with binary stress lable (>2.5)
clf_model_sleep_binary = param_tuning(X, Y_binary, clf, clf_test_params, verbose=True)

# SVM and SequentialForwardSelection
svm_linear = svm.SVC

svm_test_params = [{"C":c,"kernel":k}
                   for c in [0.01,0.1,1,2,5]
                   for k in ['linear', 'rbf','sigmoid']]

svm_model_sleep = param_tuning(X, Y, svm_linear, svm_test_params, verbose=True, sfs=True,
                                                         input_features = independent_variables)

# Test SVM model with binary stress lable (>2.5)
svm_model_sleep_binary = param_tuning(X, Y_binary, svm_linear, svm_test_params, verbose=True, sfs=True,
                                                         input_features = independent_variables)

"""## Model Building All Features Together"""

# Set up feature vectors for analysis
independent_variables = ['heart_rate_avg', 'heart_rate_max', 'heart_rate_min',
       'resting_heart_rate_avg', 'resting_heart_rate_max',
       'resting_heart_rate_min', 'heart_rate_variability_sdnn_max',
       'heart_rate_variability_sdnn_min', 'heart_rate_variability_sdnn_avg',
       'sleep_duration_hours', 'sleep_time_diff']


np.random.seed(5)
stress_features_merged = pd.merge(hr_vs_stress_df,
                                  survey_sleep_merge.drop(columns=['question', 'user_answer']).query("identifier == 'stressed'"),
                                  on=["date","userid"])

sleep_features_df = stress_features_merged[independent_variables].fillna(method='ffill',axis=0).fillna(method='bfill',axis=0)
X = sleep_features_df.to_numpy()

# Generating label based on 0-5 score or binary for stressed or not stressed
Y = stress_features_merged['user_answer'].astype(int).to_numpy()
Y_binary = Y.copy()
Y_binary[Y_binary < 2.5] = 0
Y_binary[Y_binary > 2.5] = 1

X.shape

# Run Ordinary Least Squares to obtain coefficients and Statistically Singficiant Features
X2 = sm.add_constant(X)
est = sm.OLS(Y, X2)
est2 = est.fit()
print(est2.summary())

# Create linear regression model with regularization (LASSO) and find best parameter
clf = linear_model.Lasso
clf_test_params = [{"alpha":a} for a in [0.01,0.05,0.1,0.2,0.5,0.8]]

clf_model_all = param_tuning(X, Y, clf, clf_test_params, verbose=True,
                                                         input_features = independent_variables)

# SVM and SequentialForwardSelection
svm_linear = svm.SVC

svm_test_params = [{"C":c,"kernel":k}
                   for c in [0.01,0.1,1,2,5]
                   for k in ['linear', 'rbf','sigmoid']]

svm_model_all = param_tuning(X, Y, svm_linear, svm_test_params, verbose=True, sfs=True,
                                                         input_features = independent_variables)

# Test SVM model with binary stress lable (>2.5)
svm_model_all_binary = param_tuning(X, Y_binary, svm_linear, svm_test_params, verbose=True, sfs=True,
                                                         input_features = independent_variables)

"""# More Feature Engineering
* Separate heart rate and HRV into night and day (maintain average values)
* Include feature with the heart rate ratio formula
* Include formal sleep variability index

## HR Night features and Sleep Onset HR Ratio
"""

import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

def hr_featurizer(sleep_df_section):
  # print(hr_user_df.columns)
  target_date = sleep_df_section['date'].iloc[0]
  user_name = sleep_df_section['userid'].iloc[0]

  sleep_onset_time = sleep_df_section['start_time'].iloc[0]
  sleep_end_time = sleep_df_section['end_time'].iloc[0]
  df_copy = sleep_df_section.mean()
  night_df = hr_user_df.query("date_hour > @sleep_onset_time and date_hour < @sleep_end_time and userid == @user_name").sort_values(by='date_hour')

  if len(night_df) > 0:
    df_copy['night_heart_rate_avg'] = night_df['heart_rate_avg'].mean()
    df_copy['night_heart_rate_min'] = night_df['heart_rate_min'].mean()
    df_copy['night_heart_rate_max'] = night_df['heart_rate_max'].mean()
    df_copy['onset_heart_rate_ratio'] = night_df['heart_rate_avg'].iloc[0]/night_df['heart_rate_avg'].iloc[1:].mean()
    df_copy['len_night_df'] = len(night_df)
    return df_copy
  else:
    df_copy['night_heart_rate_avg'] = None
    df_copy['night_heart_rate_min'] = None
    df_copy['night_heart_rate_max'] = None
    df_copy['onset_heart_rate_ratio'] = None
    df_copy['len_night_df'] = len(night_df)
    return df_copy

hr_night_df = processed_sleep_df.groupby(["date","userid"], as_index=False).apply(hr_featurizer)

# Set up feature vectors for analysis
independent_variables = ['heart_rate_avg', 'heart_rate_max', 'heart_rate_min',
       'resting_heart_rate_avg',
      #  'resting_heart_rate_max', 'resting_heart_rate_min',
       'heart_rate_variability_sdnn_avg',
      #  'heart_rate_variability_sdnn_max','heart_rate_variability_sdnn_min',
       'night_heart_rate_avg','night_heart_rate_min','night_heart_rate_max',
       'onset_heart_rate_ratio',
       'sleep_duration_hours',
       'sleep_time_diff'
       ]


np.random.seed(5)

expanded_survey_sleep_merge = pd.merge(hr_night_df,df_with_survey, on=["date","userid"])
stress_features_merged = pd.merge(hr_vs_stress_df,
                                  expanded_survey_sleep_merge.drop(columns=['question', 'user_answer']).query("identifier == 'stressed'"),
                                  on=["date","userid"])

sleep_features_df = stress_features_merged[independent_variables].fillna(method='ffill',axis=0).fillna(method='bfill',axis=0)
X = sleep_features_df.to_numpy()

# Generating label based on 0-5 score or binary for stressed or not stressed
Y = stress_features_merged['user_answer'].astype(int).to_numpy()
Y_binary = Y.copy()
Y_binary[Y_binary < 2.5] = 0
Y_binary[Y_binary > 2.5] = 1

X.shape

X_norm = (X - np.mean(X, axis=0))/np.std(X, axis=0)

# Run Ordinary Least Squares to obtain coefficients and Statistically Singficiant Features
corr_df = pd.DataFrame()
for feature in independent_variables:
  X2 = sm.add_constant(sleep_features_df[feature])
  est = sm.OLS(Y_binary, X2)
  est2 = est.fit()
  results_as_html = est2.summary().tables[1].as_html()
  corr_df[feature] = pd.read_html(results_as_html, header=0, index_col=0)[0].loc[feature]
corr_df = corr_df.T
feat_names = ['HR Avg','HR Max','HR Min','Resting HR Avg',
              'HRV SDNN Avg', 'Night HR Avg', "Night HR Min", "Night HR Max", "HR Onset Ratio", "Sleep Duration",
              "Sleep Time Diff."]

# fig = plt.figure(figsize=(12, 12))

# fig = sm.graphics.plot_partregress_grid(est2, fig=fig)
# fig.tight_layout(pad=1.0)

corr_df

corr_df[''] = feat_names
corr_df = corr_df.set_index('').rename(columns={'P>|t|':'p'})

corr_df

corr_df.to_csv(f"ordinary_least_squares_regression_analysis_standardized.csv")

stress_df_to_plot = stress_features_merged[['heart_rate_avg',
       'resting_heart_rate_avg', 'heart_rate_variability_sdnn_avg',
       'night_heart_rate_avg','onset_heart_rate_ratio','sleep_duration_hours',
       'sleep_time_diff'
       ] + ['user_answer']]

import matplotlib

font = {'family' : 'DejaVu Sans',
        'weight' : 'bold',
        'size'   : 12}

matplotlib.rc('font', **font)

sns_plot = sns.pairplot(stress_df_to_plot,
       kind="reg", diag_kind="kde")

# Add R-squared values to each plot
for i, j in zip(*np.triu_indices_from(sns_plot.axes, 1)):
    sns_plot.axes[i, j].annotate('$R^2 = {:.2f}$'.format(
        stress_df_to_plot.iloc[:, i].corr(stress_df_to_plot.iloc[:, j])**2),
        xy=(.1, .9), xycoords='axes fraction')

# Show the plot

# for feature, df_name in zip(corr_df.query("p < 0.08").index[1:], ['heart_rate_avg', 'heart_rate_max', 'heart_rate_variability_sdnn_avg', 'onset_heart_rate_ratio',
#        'sleep_duration_hours','sleep_time_diff']):
#   fig, axes = plt.subplots(1,2,figsize=(1?2,5))
#   axes = axes.flatten()
#   sns.regplot(x="user_answer", y=df_name, data=stress_df_to_plot,ax=axes[0])
#   axes[0].set_ylabel(feature)
#   axes[0].set_xlabel("User Answer")
#   # axes[0].set_title()
#   plt.grid()
#   sns.kdeplot(x=df_name, data=stress_df_to_plot,ax=axes[1],fill=True)
#   axes[1].set_ylabel("KDE")
#   axes[1].set_xlabel(feature)
#   plt.grid()
#   print("feature: ", feature)
#   plt.savefig(f"/content/drive/MyDrive/AI and Mental Health Project/MIT_Pilot_Analysis/Figures/{feature}.png")

# Create linear regression model with regularization (LASSO) and find best parameter
clf = linear_model.Lasso
clf_test_params = [{"alpha":a} for a in [0.01,0.05,0.1,0.2,0.5,0.8]]

clf_model_all = param_tuning(X, Y, clf, clf_test_params, verbose=True,
                                                         input_features = independent_variables)

clf_model_all_binary = param_tuning(X, Y_binary, clf, clf_test_params, verbose=True,
                                                         input_features = independent_variables)

# SVM and SequentialForwardSelection
svm_linear = svm.SVC

svm_test_params = [{"C":c,"kernel":k}
                   for c in [0.01,0.1,1,2,5]
                   for k in ['linear', 'rbf','sigmoid']]

svm_model_all = param_tuning(X, Y, svm_linear, svm_test_params, verbose=True, sfs=True,
                                                         input_features = independent_variables)

# Test SVM model with binary stress lable (>2.5)
svm_model_all_binary = param_tuning(X, Y_binary, svm_linear, svm_test_params, verbose=True, sfs=True,
                                                         input_features = independent_variables)

clf_model_all

clf_model_all_binary

svm_model_all

svm_model_all_binary

"""## HR Normalization Features"""

